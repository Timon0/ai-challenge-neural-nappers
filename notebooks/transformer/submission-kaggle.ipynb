{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 53666,
     "databundleVersionId": 6589269,
     "sourceType": "competition"
    },
    {
     "sourceId": 6980087,
     "sourceType": "datasetVersion",
     "datasetId": 4011077
    }
   ],
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from joblib import load\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Modeling\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "# Disable warnings\n",
    "pd.options.mode.chained_assignment = None"
   ],
   "metadata": {
    "is_executing": true,
    "execution": {
     "iopub.status.busy": "2023-11-16T14:27:31.713182Z",
     "iopub.execute_input": "2023-11-16T14:27:31.713553Z",
     "iopub.status.idle": "2023-11-16T14:27:31.721401Z",
     "shell.execute_reply.started": "2023-11-16T14:27:31.713525Z",
     "shell.execute_reply": "2023-11-16T14:27:31.720251Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "LAGS_FUTURE = [f\"t_lag_{i}\" for i in range(-1, -25, -1)]\n",
    "LAGS_PAST = reversed([f\"t_lag_{i}\" for i in range(1, 25)])\n",
    "FEATURES = [*LAGS_PAST, 't_0', *LAGS_FUTURE]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "execution": {
     "iopub.status.busy": "2023-11-16T14:27:31.723017Z",
     "iopub.execute_input": "2023-11-16T14:27:31.723301Z",
     "iopub.status.idle": "2023-11-16T14:27:31.734069Z",
     "shell.execute_reply.started": "2023-11-16T14:27:31.723276Z",
     "shell.execute_reply": "2023-11-16T14:27:31.733308Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "sequence_length = 24 * 60 * 12 # 17280 Steps = 1 Day\n",
    "\n",
    "def data_cleaning(series_to_clean):\n",
    "    multiplicator = 0\n",
    "\n",
    "    indices_to_remove = []\n",
    "\n",
    "    while True:\n",
    "        # Get 24 Hours (s1) and the next 24 Hours (s2)\n",
    "        s1 = series_to_clean[multiplicator*sequence_length:(multiplicator+1)*sequence_length]['anglez'].reset_index(drop=True)\n",
    "        s2 = series_to_clean[(multiplicator+1)*sequence_length:(multiplicator+2)*sequence_length]['anglez'].reset_index(drop=True)\n",
    "\n",
    "        # If the length is not the same, its the last part of the series\n",
    "        if len(s1) != len(s2):\n",
    "            # If the last part of the series is the same as the part 24 hours before, remove that as well\n",
    "            if s1[:len(s2)].equals(s2):\n",
    "                indices_to_remove.append((len(series_to_clean)-len(s2), len(series_to_clean)))\n",
    "            break\n",
    "\n",
    "        # If the 24 hours match, remove those indices\n",
    "        if s1.equals(s2):\n",
    "            indices_to_remove.append(((multiplicator+1)*sequence_length, (multiplicator+2)*sequence_length))\n",
    "\n",
    "        multiplicator += 1\n",
    "\n",
    "\n",
    "    cleaned_df = series_to_clean\n",
    "\n",
    "    # Remove the indices reversed, otherwise the indices of the remaining rows change\n",
    "    for start_idx, end_idx in reversed(indices_to_remove):\n",
    "        cleaned_df = cleaned_df.drop(index=cleaned_df.iloc[start_idx:end_idx].index)\n",
    "    \n",
    "    return cleaned_df"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-16T14:27:31.735192Z",
     "iopub.execute_input": "2023-11-16T14:27:31.735455Z",
     "iopub.status.idle": "2023-11-16T14:27:31.746477Z",
     "shell.execute_reply.started": "2023-11-16T14:27:31.735433Z",
     "shell.execute_reply": "2023-11-16T14:27:31.745737Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Normalization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#scaler = load('/kaggle/input/transformer-checkpoint/scaler.pkl')\n",
    "scaler = load('local-path/scaler.pkl')\n",
    "\n",
    "def data_normalization(series_to_normalize):\n",
    "    series_to_normalize[['enmo', 'anglez']] = scaler.transform(series_to_normalize[['enmo', 'anglez']])\n",
    "    return series_to_normalize"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-16T14:27:31.748628Z",
     "iopub.execute_input": "2023-11-16T14:27:31.749117Z",
     "iopub.status.idle": "2023-11-16T14:27:31.765770Z",
     "shell.execute_reply.started": "2023-11-16T14:27:31.749081Z",
     "shell.execute_reply": "2023-11-16T14:27:31.764845Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Engineering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def data_feature_engineering(series):\n",
    "\n",
    "    series['t_0'] = series[['anglez', 'enmo']].values.tolist()\n",
    "\n",
    "    for i in range(1, 25):\n",
    "        series[f'anglez_lag_{i}'] = series[\"anglez\"].shift(i).bfill()\n",
    "        series[f'enmo_lag_{i}'] = series[\"enmo\"].shift(i).bfill()\n",
    "        series[f't_lag_{i}'] = series[[f'anglez_lag_{i}', f'enmo_lag_{i}']].values.tolist()\n",
    "        series = series.drop(columns=[f'anglez_lag_{i}', f'enmo_lag_{i}'])\n",
    "\n",
    "    for i in range(-1, -25, -1):\n",
    "        series[f'anglez_lag_{i}'] = series[\"anglez\"].shift(i).ffill()\n",
    "        series[f'enmo_lag_{i}'] = series[\"enmo\"].shift(i).ffill()\n",
    "        series[f't_lag_{i}'] = series[[f'anglez_lag_{i}', f'enmo_lag_{i}']].values.tolist()\n",
    "        series = series.drop(columns=[f'anglez_lag_{i}', f'enmo_lag_{i}'])\n",
    "    \n",
    "    return series.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-16T14:27:31.767474Z",
     "iopub.execute_input": "2023-11-16T14:27:31.767989Z",
     "iopub.status.idle": "2023-11-16T14:27:31.776462Z",
     "shell.execute_reply.started": "2023-11-16T14:27:31.767955Z",
     "shell.execute_reply": "2023-11-16T14:27:31.775473Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def data_pipeline(series_id):\n",
    "    #series = pd.read_parquet('/kaggle/input/child-mind-institute-detect-sleep-states/train_series.parquet', filters=[('series_id','=',series_id)])\n",
    "    series = pd.read_parquet('../../data/processed/validation_series_split.parquet', filters=[('series_id','=',series_id)])\n",
    "    series = data_cleaning(series)\n",
    "    series = data_normalization(series)\n",
    "    return data_feature_engineering(series)"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-16T14:27:31.777580Z",
     "iopub.execute_input": "2023-11-16T14:27:31.777853Z",
     "iopub.status.idle": "2023-11-16T14:27:31.790797Z",
     "shell.execute_reply.started": "2023-11-16T14:27:31.777830Z",
     "shell.execute_reply": "2023-11-16T14:27:31.789859Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Definition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, n_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(d_model * seq_len, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features=2, encoder_layer_nhead=4, num_layers=2, dim_model=64, num_classes=2,\n",
    "                 sequence_length=49, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_type = 'Transformer'\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.encoder_layer_nhead = encoder_layer_nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_model = dim_model\n",
    "        self.num_classes = num_classes\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.embedding = nn.Linear(self.num_features, self.dim_model)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(self.dim_model, dropout, self.sequence_length)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.dim_model,\n",
    "                                                   nhead=self.encoder_layer_nhead,\n",
    "                                                   batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=self.num_layers)\n",
    "\n",
    "        self.classifier = ClassificationHead(seq_len=sequence_length, d_model=self.dim_model, n_classes=num_classes)\n",
    "\n",
    "    def forward(self, src):\n",
    "        output = self.embedding(src)\n",
    "        output = self.pos_encoder(output)\n",
    "        output = self.encoder(output)\n",
    "        return self.classifier(output)\n",
    "\n",
    "\n",
    "class LightningModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model=None, encoder_layer_nhead=4, num_layers=2, dim_model=64, learning_rate=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.sequence_length = 49\n",
    "        self.num_features = 2\n",
    "        self.num_classes = 2\n",
    "        self.encoder_layer_nhead = encoder_layer_nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_model = dim_model\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if model is None:\n",
    "            self.model = TransformerEncoderClassifier(self.num_features,\n",
    "                                                      self.encoder_layer_nhead,\n",
    "                                                      self.num_layers,\n",
    "                                                      self.dim_model,\n",
    "                                                      self.num_classes,\n",
    "                                                      self.sequence_length)\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "        # metrics\n",
    "        metrics = MetricCollection([\n",
    "            MulticlassAccuracy(num_classes=self.num_classes),\n",
    "            MulticlassPrecision(num_classes=self.num_classes),\n",
    "            MulticlassRecall(num_classes=self.num_classes),\n",
    "            MulticlassF1Score(num_classes=self.num_classes)\n",
    "        ])\n",
    "        self.train_metrics = metrics.clone(prefix='train_')\n",
    "        self.val_metrics = metrics.clone(prefix='val_')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, true_labels, logits = self._shared_step(batch)\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "        self.train_metrics(logits, true_labels)\n",
    "        self.log_dict(self.train_metrics, on_epoch=True, on_step=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        loss, true_labels, logits = self._shared_step(batch)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        self.val_metrics(logits, true_labels)\n",
    "        self.log_dict(self.val_metrics)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "        scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.001, total_iters=self._num_steps())\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def _shared_step(self, batch):\n",
    "        features, true_labels = batch\n",
    "        logits = self(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, true_labels)\n",
    "        return loss, true_labels, logits\n",
    "\n",
    "    def _num_steps(self):\n",
    "        train_dataloader = self.trainer.datamodule.train_dataloader()\n",
    "        dataset_size = len(train_dataloader.dataset)\n",
    "        num_steps = dataset_size * self.trainer.max_epochs // self.trainer.datamodule.batch_size\n",
    "        return num_steps\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_length: int = 5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          d_model:      dimension of embeddings\n",
    "          dropout:      randomly zeroes-out some of the input\n",
    "          max_length:   max sequence length\n",
    "        \"\"\"\n",
    "        # inherit from Module\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # create tensor of 0s\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "\n",
    "        # create position column\n",
    "        k = torch.arange(0, max_length).unsqueeze(1)\n",
    "\n",
    "        # calc divisor for positional encoding\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # calc sine on even indices\n",
    "        pe[:, 0::2] = torch.sin(k * div_term)\n",
    "\n",
    "        # calc cosine on odd indices\n",
    "        pe[:, 1::2] = torch.cos(k * div_term)\n",
    "\n",
    "        # add dimension\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # buffers are saved in state_dict but not trained by the optimizer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x:        embeddings (batch_size, seq_length, d_model)\n",
    "\n",
    "        Returns:\n",
    "                    embeddings + positional encodings (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # add positional encoding to the embeddings\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "\n",
    "        # perform dropout\n",
    "        return self.dropout(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-16T14:27:31.848274Z",
     "iopub.execute_input": "2023-11-16T14:27:31.848651Z",
     "iopub.status.idle": "2023-11-16T14:27:31.877675Z",
     "shell.execute_reply.started": "2023-11-16T14:27:31.848619Z",
     "shell.execute_reply": "2023-11-16T14:27:31.876750Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#model = LightningModel.load_from_checkpoint('/kaggle/input/transformer-checkpoint/transformer.ckpt')\n",
    "model = LightningModel.load_from_checkpoint('/local-checkpoint/transformer.ckpt')\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-16T14:27:31.879589Z",
     "iopub.execute_input": "2023-11-16T14:27:31.879934Z",
     "iopub.status.idle": "2023-11-16T14:27:32.025281Z",
     "shell.execute_reply.started": "2023-11-16T14:27:31.879902Z",
     "shell.execute_reply": "2023-11-16T14:27:32.024363Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def prediction_batch(batch):\n",
    "    X = batch\n",
    "    with torch.no_grad():\n",
    "        logits = model(X.to(device))\n",
    "    label = torch.argmax(logits, dim=-1)\n",
    "    confidence = torch.softmax(logits, dim=-1)\n",
    "    confidence_0 = confidence[:, 0]\n",
    "    confidence_1 = confidence[:, 1]\n",
    "    return label, confidence_0, confidence_1"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-16T14:27:32.026339Z",
     "iopub.execute_input": "2023-11-16T14:27:32.026619Z",
     "iopub.status.idle": "2023-11-16T14:27:32.032762Z",
     "shell.execute_reply.started": "2023-11-16T14:27:32.026596Z",
     "shell.execute_reply": "2023-11-16T14:27:32.031799Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def prediction(series):\n",
    "    predictions = series[['series_id', 'step']]\n",
    "    \n",
    "    label_list = []\n",
    "    confidence_0_list = []\n",
    "    confidence_1_list = []\n",
    "    \n",
    "    series_length, series_columns = series[FEATURES].values.shape\n",
    "\n",
    "    dataset = TensorDataset(torch.from_numpy(np.vstack(np.ravel(series[FEATURES].values))\n",
    "                                             .reshape(series_length, series_columns, 2)).to(torch.float32).to(device))\n",
    "    dataloader = DataLoader(dataset, batch_size=1024)\n",
    "    for index, batch in enumerate(dataloader): \n",
    "        label, confidence_0, confidence_1 = prediction_batch(batch[0])\n",
    "        \n",
    "        label_list.append(label)\n",
    "        confidence_0_list.append(confidence_0)\n",
    "        confidence_1_list.append(confidence_1) \n",
    "        \n",
    "    predictions['prediction_class'] = torch.cat(label_list).cpu().numpy()\n",
    "    predictions['prediction_confidence_0'] = torch.cat(confidence_0_list).cpu().numpy()\n",
    "    predictions['prediction_confidence_1'] = torch.cat(confidence_1_list).cpu().numpy()\n",
    "\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-16T14:27:32.034056Z",
     "iopub.execute_input": "2023-11-16T14:27:32.034436Z",
     "iopub.status.idle": "2023-11-16T14:27:32.045864Z",
     "shell.execute_reply.started": "2023-11-16T14:27:32.034404Z",
     "shell.execute_reply": "2023-11-16T14:27:32.044795Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Event Extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "smoothing_length = 12 * 60 # 60 Minutes\n",
    "\n",
    "def event_extraction(series):\n",
    "    events = []\n",
    "\n",
    "    series_id = series[\"series_id\"].values[0]\n",
    "               \n",
    "    series[\"confidence_awake\"] = series[\"prediction_confidence_1\"].rolling(smoothing_length, center=True).mean().bfill().ffill()\n",
    "    series[\"asleep\"] = series[\"prediction_confidence_0\"].rolling(smoothing_length, center=True).mean().bfill().ffill()\n",
    "\n",
    "    # Binarize the smoothing column\n",
    "    series[\"asleep\"] = series[\"asleep\"].round()\n",
    "\n",
    "    # Getting predicted onset and wakeup time steps\n",
    "    pred_onsets = series[series['asleep'].diff() > 0]['step'].tolist() # diff is > 0 if it changes from 0 (awake) to 1 (asleep)\n",
    "    pred_wakeups = series[series['asleep'].diff() < 0]['step'].tolist() # diff is < 0 if it changes from 1 (asleep) to 0 (awake)\n",
    "     \n",
    "    if len(pred_onsets) > 0 and len(pred_wakeups) > 0:\n",
    "\n",
    "        # Ensuring all predicted sleep periods begin and end\n",
    "        if min(pred_wakeups) < min(pred_onsets):\n",
    "            pred_wakeups = pred_wakeups[1:]\n",
    "\n",
    "        if max(pred_onsets) > max(pred_wakeups):\n",
    "            pred_onsets = pred_onsets[:-1]\n",
    "\n",
    "        # Keeping sleep periods longer than 30 minutes\n",
    "        sleep_periods = [(onset, wakeup) for onset, wakeup in zip(pred_onsets, pred_wakeups) if wakeup - onset >= 12 * 30]\n",
    "\n",
    "        for onset, wakeup in sleep_periods :\n",
    "            # Scoring using mean probability over period\n",
    "            score = 1 - series[(series['step'] >= onset) & (series['step'] < wakeup)]['score'].mean()\n",
    "\n",
    "            # Adding sleep event to dataframe\n",
    "            onset_row = {'row_id': len(events), 'series_id': series_id, 'step': onset, 'event': 'onset', 'score': score}                \n",
    "            events.append(onset_row)\n",
    "\n",
    "            wakeup_row = {'row_id': len(events), 'series_id': series_id, 'step': wakeup, 'event': 'wakeup', 'score': score}\n",
    "            events.append(wakeup_row)\n",
    "\n",
    "    return pd.DataFrame(events)\n"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-16T14:27:32.048679Z",
     "iopub.execute_input": "2023-11-16T14:27:32.049037Z",
     "iopub.status.idle": "2023-11-16T14:27:32.062765Z",
     "shell.execute_reply.started": "2023-11-16T14:27:32.049005Z",
     "shell.execute_reply": "2023-11-16T14:27:32.061658Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Execution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#df_test = pd.read_parquet('/kaggle/input/child-mind-institute-detect-sleep-states/train_series.parquet', columns=['series_id'])\n",
    "df_test = pd.read_parquet('../../data/processed/validation_series_split.parquet', columns=['series_id'])\n",
    "series_ids = df_test.series_id.unique()\n",
    "del df_test\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-16T14:27:32.063924Z",
     "iopub.execute_input": "2023-11-16T14:27:32.064274Z",
     "iopub.status.idle": "2023-11-16T14:27:47.906623Z",
     "shell.execute_reply.started": "2023-11-16T14:27:32.064232Z",
     "shell.execute_reply": "2023-11-16T14:27:47.905668Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "events_list = []\n",
    "\n",
    "for i, series_id in enumerate(series_ids):\n",
    "    print(f'Step {i+1} of {len(series_ids)}')\n",
    "    series_prepared = data_pipeline(series_id)\n",
    "    predictions = prediction(series_prepared)\n",
    "    events = event_extraction(predictions)\n",
    "\n",
    "    events_list.append(events)\n",
    "\n",
    "    del series_prepared\n",
    "    del predictions\n",
    "    gc.collect()\n",
    "\n",
    "events = pd.concat(events_list).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-16T14:27:47.907768Z",
     "iopub.execute_input": "2023-11-16T14:27:47.908085Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "submissions = events\n",
    "submissions.reset_index(inplace=True)\n",
    "submissions.rename(columns={\"index\": \"row_id\"}, inplace=True)\n",
    "submissions.to_csv('/kaggle/working/submission.csv', sep=',', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
