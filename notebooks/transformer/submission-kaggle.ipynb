{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from joblib import load\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Modeling\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "# Disable warnings\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "LAGS_FUTURE = [f\"t_lag_{i}\" for i in range(-1, -25, -1)]\n",
    "LAGS_PAST = reversed([f\"t_lag_{i}\" for i in range(1, 25)])\n",
    "FEATURES = [*LAGS_PAST, 't_0', *LAGS_FUTURE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 24 * 60 * 12 # 17280 Steps = 1 Day\n",
    "\n",
    "def data_cleaning(series_to_clean):\n",
    "    multiplicator = 0\n",
    "\n",
    "    indices_to_remove = []\n",
    "\n",
    "    while True:\n",
    "        # Get 24 Hours (s1) and the next 24 Hours (s2)\n",
    "        s1 = series_to_clean[multiplicator*sequence_length:(multiplicator+1)*sequence_length]['anglez'].reset_index(drop=True)\n",
    "        s2 = series_to_clean[(multiplicator+1)*sequence_length:(multiplicator+2)*sequence_length]['anglez'].reset_index(drop=True)\n",
    "\n",
    "        # If the length is not the same, its the last part of the series\n",
    "        if len(s1) != len(s2):\n",
    "            # If the last part of the series is the same as the part 24 hours before, remove that as well\n",
    "            if s1[:len(s2)].equals(s2):\n",
    "                indices_to_remove.append((len(series_to_clean)-len(s2), len(series_to_clean)))\n",
    "            break\n",
    "\n",
    "        # If the 24 hours match, remove those indices\n",
    "        if s1.equals(s2):\n",
    "            indices_to_remove.append(((multiplicator+1)*sequence_length, (multiplicator+2)*sequence_length))\n",
    "\n",
    "        multiplicator += 1\n",
    "\n",
    "\n",
    "    cleaned_df = series_to_clean\n",
    "\n",
    "    # Remove the indices reversed, otherwise the indices of the remaining rows change\n",
    "    for start_idx, end_idx in reversed(indices_to_remove):\n",
    "        cleaned_df = cleaned_df.drop(index=cleaned_df.iloc[start_idx:end_idx].index)\n",
    "    \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#scaler = load('/kaggle/input/transformer-checkpoint/scaler.pkl')\n",
    "scaler = load('../../data/processed/scaler.pkl')\n",
    "\n",
    "def data_normalization(series_to_normalize):\n",
    "    series_to_normalize[['enmo', 'anglez']] = scaler.transform(series_to_normalize[['enmo', 'anglez']])\n",
    "    return series_to_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_feature_engineering(series):\n",
    "\n",
    "    series['t_0'] = series[['anglez', 'enmo']].values.tolist()\n",
    "\n",
    "    for i in range(1, 25):\n",
    "        series[f'anglez_lag_{i}'] = series[\"anglez\"].shift(i).bfill()\n",
    "        series[f'enmo_lag_{i}'] = series[\"enmo\"].shift(i).bfill()\n",
    "        series[f't_lag_{i}'] = series[[f'anglez_lag_{i}', f'enmo_lag_{i}']].values.tolist()\n",
    "        series = series.drop(columns=[f'anglez_lag_{i}', f'enmo_lag_{i}'])\n",
    "\n",
    "    for i in range(-1, -25, -1):\n",
    "        series[f'anglez_lag_{i}'] = series[\"anglez\"].shift(i).ffill()\n",
    "        series[f'enmo_lag_{i}'] = series[\"enmo\"].shift(i).ffill()\n",
    "        series[f't_lag_{i}'] = series[[f'anglez_lag_{i}', f'enmo_lag_{i}']].values.tolist()\n",
    "        series = series.drop(columns=[f'anglez_lag_{i}', f'enmo_lag_{i}'])\n",
    "    \n",
    "    return series.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(series_id):\n",
    "    #series = pd.read_parquet('/kaggle/input/child-mind-institute-detect-sleep-states/test_series.parquet', filters=[('series_id','=',series_id)])\n",
    "    series = pd.read_parquet('../../data/processed/validation_series_split.parquet', filters=[('series_id','=',series_id)])\n",
    "    series = data_cleaning(series)\n",
    "    series = data_normalization(series)\n",
    "    return data_feature_engineering(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, n_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(d_model * seq_len, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features=2, encoder_layer_nhead=4, num_layers=2, dim_model=64, num_classes=2,\n",
    "                 sequence_length=49, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_type = 'Transformer'\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.encoder_layer_nhead = encoder_layer_nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_model = dim_model\n",
    "        self.num_classes = num_classes\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.embedding = nn.Linear(self.num_features, self.dim_model)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(self.dim_model, dropout, self.sequence_length)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.dim_model,\n",
    "                                                   nhead=self.encoder_layer_nhead,\n",
    "                                                   batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=self.num_layers)\n",
    "\n",
    "        self.classifier = ClassificationHead(seq_len=sequence_length, d_model=self.dim_model, n_classes=num_classes)\n",
    "\n",
    "    def forward(self, src):\n",
    "        output = self.embedding(src)\n",
    "        output = self.pos_encoder(output)\n",
    "        output = self.encoder(output)\n",
    "        return self.classifier(output)\n",
    "\n",
    "\n",
    "class LightningModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model=None, encoder_layer_nhead=4, num_layers=2, dim_model=64, learning_rate=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.sequence_length = 49\n",
    "        self.num_features = 2\n",
    "        self.num_classes = 2\n",
    "        self.encoder_layer_nhead = encoder_layer_nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_model = dim_model\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if model is None:\n",
    "            self.model = TransformerEncoderClassifier(self.num_features,\n",
    "                                                      self.encoder_layer_nhead,\n",
    "                                                      self.num_layers,\n",
    "                                                      self.dim_model,\n",
    "                                                      self.num_classes,\n",
    "                                                      self.sequence_length)\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "        # metrics\n",
    "        metrics = MetricCollection([\n",
    "            MulticlassAccuracy(num_classes=self.num_classes),\n",
    "            MulticlassPrecision(num_classes=self.num_classes),\n",
    "            MulticlassRecall(num_classes=self.num_classes),\n",
    "            MulticlassF1Score(num_classes=self.num_classes)\n",
    "        ])\n",
    "        self.train_metrics = metrics.clone(prefix='train_')\n",
    "        self.val_metrics = metrics.clone(prefix='val_')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, true_labels, logits = self._shared_step(batch)\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "        self.train_metrics(logits, true_labels)\n",
    "        self.log_dict(self.train_metrics, on_epoch=True, on_step=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        loss, true_labels, logits = self._shared_step(batch)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        self.val_metrics(logits, true_labels)\n",
    "        self.log_dict(self.val_metrics)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "        scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.001, total_iters=self._num_steps())\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def _shared_step(self, batch):\n",
    "        features, true_labels = batch\n",
    "        logits = self(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, true_labels)\n",
    "        return loss, true_labels, logits\n",
    "\n",
    "    def _num_steps(self):\n",
    "        train_dataloader = self.trainer.datamodule.train_dataloader()\n",
    "        dataset_size = len(train_dataloader.dataset)\n",
    "        num_steps = dataset_size * self.trainer.max_epochs // self.trainer.datamodule.batch_size\n",
    "        return num_steps\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_length: int = 5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          d_model:      dimension of embeddings\n",
    "          dropout:      randomly zeroes-out some of the input\n",
    "          max_length:   max sequence length\n",
    "        \"\"\"\n",
    "        # inherit from Module\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # create tensor of 0s\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "\n",
    "        # create position column\n",
    "        k = torch.arange(0, max_length).unsqueeze(1)\n",
    "\n",
    "        # calc divisor for positional encoding\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # calc sine on even indices\n",
    "        pe[:, 0::2] = torch.sin(k * div_term)\n",
    "\n",
    "        # calc cosine on odd indices\n",
    "        pe[:, 1::2] = torch.cos(k * div_term)\n",
    "\n",
    "        # add dimension\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # buffers are saved in state_dict but not trained by the optimizer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x:        embeddings (batch_size, seq_length, d_model)\n",
    "\n",
    "        Returns:\n",
    "                    embeddings + positional encodings (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # add positional encoding to the embeddings\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "\n",
    "        # perform dropout\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LightningModel(\n",
       "  (model): TransformerEncoderClassifier(\n",
       "    (embedding): Linear(in_features=2, out_features=64, bias=True)\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): ClassificationHead(\n",
       "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (seq): Sequential(\n",
       "        (0): Flatten(start_dim=1, end_dim=-1)\n",
       "        (1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (4): ReLU()\n",
       "        (5): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (6): ReLU()\n",
       "        (7): Linear(in_features=128, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (train_metrics): MetricCollection(\n",
       "    (MulticlassAccuracy): MulticlassAccuracy()\n",
       "    (MulticlassPrecision): MulticlassPrecision()\n",
       "    (MulticlassRecall): MulticlassRecall()\n",
       "    (MulticlassF1Score): MulticlassF1Score(),\n",
       "    prefix=train_\n",
       "  )\n",
       "  (val_metrics): MetricCollection(\n",
       "    (MulticlassAccuracy): MulticlassAccuracy()\n",
       "    (MulticlassPrecision): MulticlassPrecision()\n",
       "    (MulticlassRecall): MulticlassRecall()\n",
       "    (MulticlassF1Score): MulticlassF1Score(),\n",
       "    prefix=val_\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = LightningModel.load_from_checkpoint('/kaggle/input/transformer-checkpoint/transformer.ckpt')\n",
    "model = LightningModel.load_from_checkpoint('../../models/transformer/neural-nappers/pei7u45k/checkpoints/transformer.ckpt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_batch(batch):\n",
    "    X = batch\n",
    "    with torch.no_grad():\n",
    "        logits = model(X.to(device))\n",
    "    label = torch.argmax(logits, dim=-1)\n",
    "    confidence = torch.softmax(logits, dim=-1)\n",
    "    confidence_0 = confidence[:, 0]\n",
    "    confidence_1 = confidence[:, 1]\n",
    "    return label, confidence_0, confidence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(series):\n",
    "    predictions = series[['series_id', 'step']]\n",
    "    \n",
    "    label_list = []\n",
    "    confidence_0_list = []\n",
    "    confidence_1_list = []\n",
    "    \n",
    "    series_length, series_columns = series[FEATURES].values.shape\n",
    "    print(f'Series-length: {series_length}')\n",
    "    start_time = time()\n",
    "    dataset = TensorDataset(torch.from_numpy(np.vstack(np.ravel(series[FEATURES].values))\n",
    "                                             .reshape(series_length, series_columns, 2)).to(torch.float32).to(device))\n",
    "    \n",
    "    print(f'Creation of dataset took {(time() - start_time):.2f} Seconds')\n",
    "    dataloader = DataLoader(dataset, batch_size=10000)\n",
    "    start_time = time()\n",
    "    for index, batch in enumerate(dataloader):\n",
    "        print(f'Batch Nr. {index + 1}')\n",
    "        label, confidence_0, confidence_1 = prediction_batch(batch[0])\n",
    "        \n",
    "        label_list.append(label)\n",
    "        confidence_0_list.append(confidence_0)\n",
    "        confidence_1_list.append(confidence_1) \n",
    "\n",
    "    print(f'Prediction took {(time() - start_time):.2f} Seconds')\n",
    "    predictions['prediction_class'] = torch.cat(label_list).cpu().numpy()\n",
    "    predictions['prediction_confidence_0'] = torch.cat(confidence_0_list).cpu().numpy()\n",
    "    predictions['prediction_confidence_1'] = torch.cat(confidence_1_list).cpu().numpy()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_length = 12 * 60 # 60 Minutes\n",
    "\n",
    "def event_extraction(series):\n",
    "    events = []\n",
    "\n",
    "    series_id = series[\"series_id\"].values[0]\n",
    "               \n",
    "    series[\"confidence_awake\"] = series[\"prediction_confidence_1\"].rolling(smoothing_length, center=True).mean().bfill().ffill()\n",
    "    series[\"asleep\"] = series[\"prediction_confidence_0\"].rolling(smoothing_length, center=True).mean().bfill().ffill()\n",
    "\n",
    "    # Binarize the smoothing column\n",
    "    series[\"asleep\"] = series[\"asleep\"].round()\n",
    "\n",
    "    # Getting predicted onset and wakeup time steps\n",
    "    pred_onsets = series[series['asleep'].diff() > 0]['step'].tolist() # diff is > 0 if it changes from 0 (awake) to 1 (asleep)\n",
    "    pred_wakeups = series[series['asleep'].diff() < 0]['step'].tolist() # diff is < 0 if it changes from 1 (asleep) to 0 (awake)\n",
    "     \n",
    "    if len(pred_onsets) > 0 and len(pred_wakeups) > 0:\n",
    "\n",
    "        # Ensuring all predicted sleep periods begin and end\n",
    "        if min(pred_wakeups) < min(pred_onsets):\n",
    "            pred_wakeups = pred_wakeups[1:]\n",
    "\n",
    "        if max(pred_onsets) > max(pred_wakeups):\n",
    "            pred_onsets = pred_onsets[:-1]\n",
    "\n",
    "        # Keeping sleep periods longer than 30 minutes\n",
    "        sleep_periods = [(onset, wakeup) for onset, wakeup in zip(pred_onsets, pred_wakeups) if wakeup - onset >= 12 * 30]\n",
    "\n",
    "        for onset, wakeup in sleep_periods :\n",
    "            # Scoring using mean probability over period\n",
    "            score = 1 - series[(series['step'] >= onset) & (series['step'] < wakeup)]['score'].mean()\n",
    "\n",
    "            # Adding sleep event to dataframe\n",
    "            onset_row = {'row_id': len(events), 'series_id': series_id, 'step': onset, 'event': 'onset', 'score': score}                \n",
    "            events.append(onset_row)\n",
    "\n",
    "            wakeup_row = {'row_id': len(events), 'series_id': series_id, 'step': wakeup, 'event': 'wakeup', 'score': score}\n",
    "            events.append(wakeup_row)\n",
    "\n",
    "    return pd.DataFrame(events)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_test = pd.read_parquet('/kaggle/input/child-mind-institute-detect-sleep-states/test_series.parquet', columns=['series_id'])\n",
    "df_test = pd.read_parquet('../../data/processed/validation_series_split.parquet', columns=['series_id'])\n",
    "series_ids = df_test.series_id.unique()[:5]\n",
    "del df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 of 5\n",
      "Series-length: 744120\n",
      "Creation of dataset took 54.02 Seconds\n",
      "Batch Nr. 1\n",
      "Batch Nr. 2\n",
      "Batch Nr. 3\n",
      "Batch Nr. 4\n",
      "Batch Nr. 5\n",
      "Batch Nr. 6\n",
      "Batch Nr. 7\n",
      "Batch Nr. 8\n",
      "Batch Nr. 9\n",
      "Batch Nr. 10\n",
      "Batch Nr. 11\n",
      "Batch Nr. 12\n",
      "Batch Nr. 13\n",
      "Batch Nr. 14\n",
      "Batch Nr. 15\n",
      "Batch Nr. 16\n",
      "Batch Nr. 17\n",
      "Batch Nr. 18\n",
      "Batch Nr. 19\n",
      "Batch Nr. 20\n",
      "Batch Nr. 21\n",
      "Batch Nr. 22\n",
      "Batch Nr. 23\n",
      "Batch Nr. 24\n",
      "Batch Nr. 25\n",
      "Batch Nr. 26\n",
      "Batch Nr. 27\n",
      "Batch Nr. 28\n",
      "Batch Nr. 29\n",
      "Batch Nr. 30\n",
      "Batch Nr. 31\n",
      "Batch Nr. 32\n",
      "Batch Nr. 33\n",
      "Batch Nr. 34\n",
      "Batch Nr. 35\n",
      "Batch Nr. 36\n",
      "Batch Nr. 37\n",
      "Batch Nr. 38\n",
      "Batch Nr. 39\n",
      "Batch Nr. 40\n",
      "Batch Nr. 41\n",
      "Batch Nr. 42\n",
      "Batch Nr. 43\n",
      "Batch Nr. 44\n",
      "Batch Nr. 45\n",
      "Batch Nr. 46\n",
      "Batch Nr. 47\n",
      "Batch Nr. 48\n",
      "Batch Nr. 49\n",
      "Batch Nr. 50\n",
      "Batch Nr. 51\n",
      "Batch Nr. 52\n",
      "Batch Nr. 53\n",
      "Batch Nr. 54\n",
      "Batch Nr. 55\n",
      "Batch Nr. 56\n",
      "Batch Nr. 57\n",
      "Batch Nr. 58\n",
      "Batch Nr. 59\n",
      "Batch Nr. 60\n",
      "Batch Nr. 61\n",
      "Batch Nr. 62\n",
      "Batch Nr. 63\n",
      "Batch Nr. 64\n",
      "Batch Nr. 65\n",
      "Batch Nr. 66\n",
      "Batch Nr. 67\n",
      "Batch Nr. 68\n",
      "Batch Nr. 69\n",
      "Batch Nr. 70\n",
      "Batch Nr. 71\n",
      "Batch Nr. 72\n",
      "Batch Nr. 73\n",
      "Batch Nr. 74\n",
      "Batch Nr. 75\n",
      "Prediction took 41.03 Seconds\n",
      "Step 2 of 5\n",
      "Series-length: 276480\n",
      "Creation of dataset took 19.67 Seconds\n",
      "Batch Nr. 1\n",
      "Batch Nr. 2\n",
      "Batch Nr. 3\n",
      "Batch Nr. 4\n",
      "Batch Nr. 5\n",
      "Batch Nr. 6\n",
      "Batch Nr. 7\n",
      "Batch Nr. 8\n",
      "Batch Nr. 9\n",
      "Batch Nr. 10\n",
      "Batch Nr. 11\n",
      "Batch Nr. 12\n",
      "Batch Nr. 13\n",
      "Batch Nr. 14\n",
      "Batch Nr. 15\n",
      "Batch Nr. 16\n",
      "Batch Nr. 17\n",
      "Batch Nr. 18\n",
      "Batch Nr. 19\n",
      "Batch Nr. 20\n",
      "Batch Nr. 21\n",
      "Batch Nr. 22\n",
      "Batch Nr. 23\n",
      "Batch Nr. 24\n",
      "Batch Nr. 25\n",
      "Batch Nr. 26\n",
      "Batch Nr. 27\n",
      "Batch Nr. 28\n",
      "Prediction took 11.48 Seconds\n",
      "Step 3 of 5\n",
      "Series-length: 362880\n",
      "Creation of dataset took 26.06 Seconds\n",
      "Batch Nr. 1\n",
      "Batch Nr. 2\n",
      "Batch Nr. 3\n",
      "Batch Nr. 4\n",
      "Batch Nr. 5\n",
      "Batch Nr. 6\n",
      "Batch Nr. 7\n",
      "Batch Nr. 8\n",
      "Batch Nr. 9\n",
      "Batch Nr. 10\n",
      "Batch Nr. 11\n",
      "Batch Nr. 12\n",
      "Batch Nr. 13\n",
      "Batch Nr. 14\n",
      "Batch Nr. 15\n",
      "Batch Nr. 16\n",
      "Batch Nr. 17\n",
      "Batch Nr. 18\n",
      "Batch Nr. 19\n",
      "Batch Nr. 20\n",
      "Batch Nr. 21\n",
      "Batch Nr. 22\n",
      "Batch Nr. 23\n",
      "Batch Nr. 24\n",
      "Batch Nr. 25\n",
      "Batch Nr. 26\n",
      "Batch Nr. 27\n",
      "Batch Nr. 28\n",
      "Batch Nr. 29\n",
      "Batch Nr. 30\n",
      "Batch Nr. 31\n",
      "Batch Nr. 32\n",
      "Batch Nr. 33\n",
      "Batch Nr. 34\n",
      "Batch Nr. 35\n",
      "Batch Nr. 36\n",
      "Batch Nr. 37\n",
      "Prediction took 17.02 Seconds\n",
      "Step 4 of 5\n",
      "Series-length: 392400\n",
      "Creation of dataset took 28.49 Seconds\n",
      "Batch Nr. 1\n",
      "Batch Nr. 2\n",
      "Batch Nr. 3\n",
      "Batch Nr. 4\n",
      "Batch Nr. 5\n",
      "Batch Nr. 6\n",
      "Batch Nr. 7\n",
      "Batch Nr. 8\n",
      "Batch Nr. 9\n",
      "Batch Nr. 10\n",
      "Batch Nr. 11\n",
      "Batch Nr. 12\n",
      "Batch Nr. 13\n",
      "Batch Nr. 14\n",
      "Batch Nr. 15\n",
      "Batch Nr. 16\n",
      "Batch Nr. 17\n",
      "Batch Nr. 18\n",
      "Batch Nr. 19\n",
      "Batch Nr. 20\n",
      "Batch Nr. 21\n",
      "Batch Nr. 22\n",
      "Batch Nr. 23\n",
      "Batch Nr. 24\n",
      "Batch Nr. 25\n",
      "Batch Nr. 26\n",
      "Batch Nr. 27\n",
      "Batch Nr. 28\n",
      "Batch Nr. 29\n",
      "Batch Nr. 30\n",
      "Batch Nr. 31\n",
      "Batch Nr. 32\n",
      "Batch Nr. 33\n",
      "Batch Nr. 34\n",
      "Batch Nr. 35\n",
      "Batch Nr. 36\n",
      "Batch Nr. 37\n",
      "Batch Nr. 38\n",
      "Batch Nr. 39\n",
      "Batch Nr. 40\n",
      "Prediction took 18.91 Seconds\n",
      "Step 5 of 5\n",
      "Series-length: 120960\n",
      "Creation of dataset took 8.74 Seconds\n",
      "Batch Nr. 1\n",
      "Batch Nr. 2\n",
      "Batch Nr. 3\n",
      "Batch Nr. 4\n",
      "Batch Nr. 5\n",
      "Batch Nr. 6\n",
      "Batch Nr. 7\n",
      "Batch Nr. 8\n",
      "Batch Nr. 9\n",
      "Batch Nr. 10\n",
      "Batch Nr. 11\n",
      "Batch Nr. 12\n",
      "Batch Nr. 13\n",
      "Prediction took 2.01 Seconds\n"
     ]
    }
   ],
   "source": [
    "events_list = []\n",
    "\n",
    "for i, series_id in enumerate(series_ids):\n",
    "    print(f'Step {i+1} of {len(series_ids)}')\n",
    "    series_prepared = data_pipeline(series_id)\n",
    "    predictions = prediction(series_prepared)\n",
    "    events = event_extraction(predictions)\n",
    "\n",
    "    events_list.append(events)\n",
    "\n",
    "    del series_prepared\n",
    "    del predictions\n",
    "    gc.collect()\n",
    "\n",
    "events = pd.concat(events_list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = events\n",
    "submissions.reset_index(inplace=True)\n",
    "submissions.rename(columns={\"index\": \"row_id\"}, inplace=True)\n",
    "submissions.to_csv('submission.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6589269,
     "sourceId": 53666,
     "sourceType": "competition"
    },
    {
     "datasetId": 4011077,
     "sourceId": 6980087,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
