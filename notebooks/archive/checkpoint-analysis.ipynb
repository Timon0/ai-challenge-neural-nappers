{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95ce2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32518dd",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "42057214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_in_channels: int, conv_out_channels: int, conv_kernel_size: int, conv_stride: int, pool_kernel_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=conv_in_channels,\n",
    "                      out_channels=conv_out_channels,\n",
    "                      kernel_size=conv_kernel_size,\n",
    "                      stride=conv_stride),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool1d(pool_kernel_size),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class PyTorchFCN(torch.nn.Module):\n",
    "    \"\"\"A PyTorch implementation of the FCN Baseline\n",
    "    From https://arxiv.org/abs/1909.04939\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    sequence_length:\n",
    "        The size of the input sequence\n",
    "    num_pred_classes:\n",
    "        The number of output classes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 2, num_pred_classes: int = 2) -> None:\n",
    "        super(PyTorchFCN, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*[\n",
    "            ConvBlock(in_channels, 1024, 8, 1, 3),\n",
    "            ConvBlock(1024, 516, 5, 1, 3),\n",
    "            ConvBlock(516, 256, 3, 1, 3),\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(*[\n",
    "            nn.Linear(9216, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_pred_classes)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class LightningModel(L.LightningModule):\n",
    "\n",
    "    def __init__(self, model=None, hidden_dim=None, num_hidden_layers=None, learning_rate=None, seq_len=None):\n",
    "        super(LightningModel, self).__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.num_features = 2\n",
    "        self.num_classes = 2\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seq_len = 49\n",
    "\n",
    "        # model\n",
    "        if model is None:\n",
    "            self.model = PyTorchFCN(self.num_features, self.num_classes)\n",
    "\n",
    "        # metrics\n",
    "        metrics = MetricCollection([\n",
    "            MulticlassAccuracy(num_classes=self.num_classes),\n",
    "            MulticlassPrecision(num_classes=self.num_classes),\n",
    "            MulticlassRecall(num_classes=self.num_classes),\n",
    "            MulticlassF1Score(num_classes=self.num_classes)\n",
    "        ])\n",
    "        self.train_metrics = metrics.clone(prefix='train_')\n",
    "        self.val_metrics = metrics.clone(prefix='val_')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        loss, true_labels, logits = self._shared_step(batch)\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "        self.train_metrics(logits, true_labels)\n",
    "        self.log_dict(self.train_metrics, on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def training_step_end(self, training_step_outputs):\n",
    "        return {'loss': training_step_outputs['loss'].sum()}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        loss, true_labels, logits = self._shared_step(batch)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "        self.val_metrics(logits, true_labels)\n",
    "        self.log_dict(self.val_metrics)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _shared_step(self, batch):\n",
    "        features, true_labels = batch\n",
    "\n",
    "        logits = self(features)\n",
    "        loss = F.cross_entropy(logits, true_labels)\n",
    "        return loss, true_labels, logits\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "        scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.001, total_iters=self._num_steps())\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def _num_steps(self) -> int:\n",
    "        \"\"\"Get number of steps\"\"\"\n",
    "        train_dataloader = self.trainer.datamodule.train_dataloader()\n",
    "        dataset_size = len(train_dataloader.dataset)\n",
    "        num_steps = dataset_size * self.trainer.max_epochs // self.trainer.datamodule.batch_size\n",
    "        return num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d05b6f",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a9b0ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './chechpoints-gpuhub/neural-nappers/psd3mcwl/checkpoints/epoch=4-step=730605.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f9dbd54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "dbbc0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightningModel.load_from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d231b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7c29d10c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model.layers.0.layers.0.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.layers.0.layers.0.weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(state_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.layers.1.layers.0.weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'model.layers.0.layers.0.weight'"
     ]
    }
   ],
   "source": [
    "state_dict = checkpoint['state_dict']\n",
    "print(state_dict['model.layers.0.layers.0.weight'].shape)\n",
    "print(state_dict['model.layers.1.layers.0.weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a0868c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['model.conv_layers.0.layers.0.weight', 'model.conv_layers.0.layers.0.bias', 'model.conv_layers.1.layers.0.weight', 'model.conv_layers.1.layers.0.bias', 'model.classifier.0.weight', 'model.classifier.0.bias', 'model.classifier.2.weight', 'model.classifier.2.bias'])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890900bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-challenge",
   "language": "python",
   "name": "ai-challenge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
