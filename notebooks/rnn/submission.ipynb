{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "sys.path.append('../../')\n",
    "\n",
    "from scoring.event_detection_matrix import competition_score\n",
    "from models import PyTorchMLP, LightningModel\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"../../models/neural-nappers/wv1kmbtj/checkpoints/epoch=4-step=871060.ckpt\"\n",
    "model = LightningModel.load_from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_events = pd.read_csv('../../data/processed/validation_events_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAGS_FUTURE = [f\"t_lag_{i}\" for i in range(-1, -25, -1)]\n",
    "LAGS_PAST = reversed([f\"t_lag_{i}\" for i in range(1, 25)])\n",
    "FEATURES = [*LAGS_PAST, 't_0', *LAGS_FUTURE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunk(num_series_id, chunk):\n",
    "    series_length, series_columns = chunk[FEATURES].values.shape\n",
    "    X = torch.from_numpy(np.vstack(np.ravel(chunk[FEATURES].values))\n",
    "                           .reshape(series_length, series_columns, 2)).to(torch.float32)\n",
    "    torch.save(X, './data/' + str(num_series_id) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_chunk(num_series_id):\n",
    "    df = pd.read_parquet('../../data/processed/validation_series_split.parquet', filters=[('series_id','=',num_series_id)])\n",
    "    df['t_0'] = df[['anglez', 'enmo']].values.tolist()\n",
    "\n",
    "    for i in range(1, 25):\n",
    "        df[f'anglez_lag_{i}'] = df[\"anglez\"].shift(i).bfill()\n",
    "        df[f'enmo_lag_{i}'] = df[\"enmo\"].shift(i).bfill()\n",
    "        df[f't_lag_{i}'] = df[[f'anglez_lag_{i}', f'enmo_lag_{i}']].values.tolist()\n",
    "        df = df.drop(columns=[f'anglez_lag_{i}', f'enmo_lag_{i}'])\n",
    "\n",
    "    for i in range(-1, -25, -1):\n",
    "        df[f'anglez_lag_{i}'] = df[\"anglez\"].shift(i).ffill()\n",
    "        df[f'enmo_lag_{i}'] = df[\"enmo\"].shift(i).ffill()\n",
    "        df[f't_lag_{i}'] = df[[f'anglez_lag_{i}', f'enmo_lag_{i}']].values.tolist()\n",
    "        df = df.drop(columns=[f'anglez_lag_{i}', f'enmo_lag_{i}'])\n",
    "    \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(series):\n",
    "    overview_data = []\n",
    "\n",
    "    for num_series_id in tqdm(series.series_id.unique()):\n",
    "        chunk = make_features_chunk(num_series_id)\n",
    "        save_chunk(num_series_id, chunk)\n",
    "\n",
    "        overview_data.append(\n",
    "            chunk[['series_id', 'step']].reset_index().rename(columns={'index':'series_index'}).copy()[['series_id', 'step', 'series_index']]\n",
    "        )\n",
    "\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    return pd.concat(overview_data).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = make_features(df_validation_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events_smoothed(test_series) :\n",
    "    series_ids = test_series['series_id'].unique()\n",
    "    events = []\n",
    "\n",
    "    for idx in tqdm(series_ids):\n",
    "        # Collecting sample and normalizing features\n",
    "        X = test_series[test_series.series_id == idx]\n",
    "                \n",
    "        smoothing_length = 12 * 30 # 30 Minutes\n",
    "        X[\"score\"] = X[\"prediction_confidence_1\"].rolling(smoothing_length, center=True).mean().bfill().ffill()\n",
    "        X[\"smooth\"] = X[\"prediction_confidence_0\"].rolling(smoothing_length, center=True).mean().bfill().ffill()\n",
    "\n",
    "        # Binarize the smoothing column\n",
    "        X[\"smooth\"] = X[\"smooth\"].round()\n",
    "\n",
    "        # Getting predicted onset and wakeup time steps\n",
    "        pred_onsets = X[X['smooth'].diff() > 0]['step'].tolist()\n",
    "        pred_wakeups = X[X['smooth'].diff() < 0]['step'].tolist()\n",
    "     \n",
    "        if len(pred_onsets) > 0 : \n",
    "\n",
    "            # Ensuring all predicted sleep periods begin and end\n",
    "            if min(pred_wakeups) < min(pred_onsets) : \n",
    "                pred_wakeups = pred_wakeups[1:]\n",
    "\n",
    "            if max(pred_onsets) > max(pred_wakeups) :\n",
    "                pred_onsets = pred_onsets[:-1]\n",
    "\n",
    "            # Keeping sleep periods longer than 30 minutes\n",
    "            sleep_periods = [(onset, wakeup) for onset, wakeup in zip(pred_onsets, pred_wakeups) if wakeup - onset >= 12 * 30]\n",
    "\n",
    "            for onset, wakeup in sleep_periods :\n",
    "                # Scoring using mean probability over period\n",
    "                score = X[(X['step'] >= onset) & (X['step'] <= wakeup)]['score'].mean()\n",
    "\n",
    "                # Adding sleep event to dataframe\n",
    "                onset_row = {'row_id': len(events), 'series_id': idx, 'step': onset, 'event': 'onset', 'score': score}                \n",
    "                events.append(onset_row)\n",
    "\n",
    "                wakeup_row = {'row_id': len(events), 'series_id': idx, 'step': wakeup, 'event': 'wakeup', 'score': score}\n",
    "                events.append(wakeup_row)\n",
    "\n",
    "    return pd.DataFrame(events)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aich",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
